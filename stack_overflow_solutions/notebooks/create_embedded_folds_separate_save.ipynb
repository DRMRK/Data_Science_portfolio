{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we use the fasttext embeddings and convert our sentences into 300 dimensional vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folds.py\n",
    "# import pandas and model_selection module from scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "import re, string\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import time\n",
    "\n",
    "#punctuation = list(string.punctuation)\n",
    "\n",
    "my_stopwords = nltk.corpus.stopwords.words('english')# punctuation\n",
    "\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet =str(tweet)\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "def tokenize(s):\n",
    "    s=str(s)\n",
    "    output = re.sub(r'\\d+', '', s) # remove numbers \n",
    "    output = remove_links(output)\n",
    "    result = re_tok.sub(r' \\1 ', output).split() \n",
    "    result = [word for word in result if len(word)>2]\n",
    "    result = [word for word in result if word not in my_stopwords]\n",
    "    return result\n",
    "\n",
    "# convert sentences to vectors from embedding, embedding is 300 dimensional\n",
    "def sentence_to_vec(s,embedding_dict,stop_words,tokenizer):\n",
    "    \"\"\"\n",
    "    s: sentence, string\n",
    "    embedding_dict: dictionary word: vector\n",
    "    stop_words: list of stop words\n",
    "    tokenizer: tokenizer function\n",
    "    \"\"\"\n",
    "    # convert sentence to string and lowercase it\n",
    "    # words = str(s).lower()\n",
    "    # tokenize the sentence\n",
    "    words = s\n",
    "    words = tokenizer(words)\n",
    "    # remove stop words\n",
    "    # words =[w for w in words if not w in stop_words]\n",
    "    \n",
    "    # keep only alpha numeric tokens\n",
    "    words =[w for w in words if w.isalpha()]\n",
    "    # initialize empty list to store embeddings\n",
    "    M = []\n",
    "    for w in words:\n",
    "        # for every word, get the embedding from the dictionary\n",
    "        # and append to the list of embeddings\n",
    "        if w in embedding_dict:\n",
    "            M.append(embedding_dict[w])\n",
    "    # if we don't have any vectors return zeros\n",
    "    if len(M)==0:\n",
    "        return np.zeros(300)\n",
    "    # convert list of embeddings to array\n",
    "    M = np.array(M)\n",
    "    # calculate sum over axis=0\n",
    "    v = M.sum(axis=0)\n",
    "    return v/np.sqrt((v**2).sum())       \n",
    "\n",
    "\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(\n",
    "        fname,'r',encoding ='utf-8',\n",
    "        newline = '\\n',\n",
    "        errors='ignore'\n",
    "        )\n",
    "    n,d = map(int,fin.readline().split())\n",
    "    data ={}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]]=list(map(float,tokens[1:]))\n",
    "    return data    \n",
    "\n",
    "\n",
    "#create sentence embeddings\n",
    "def get_vectors(vectors,column,embeddings):\n",
    "    \"\"\"\n",
    "    vectors: empty array to fill in\n",
    "    column: dataframe column\n",
    "    return embedding vestors\n",
    "    \"\"\"\n",
    "    for body in column:\n",
    "        vectors.append(\n",
    "            sentence_to_vec(s= body,\n",
    "                           embedding_dict = embeddings,\n",
    "                           stop_words =my_stopwords,\n",
    "                           tokenizer=tokenize\n",
    "                           )\n",
    "        )\n",
    "    return vectors   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings\n",
      "time to load 93.55019402503967\n",
      " \n",
      "time to read 30.620375394821167\n",
      " \n",
      "creating sentence embedding\n",
      "the BodyMarkDown column embedding\n",
      "1st column shape (3370528, 300)\n",
      "time to BodyMarkDown 2627.3974480628967\n",
      " \n",
      "the Title column embedding\n",
      "2nd column shape (3370528, 300)\n",
      "time to Title 272.0837643146515\n",
      " \n",
      "embeddings 83886168\n",
      "vector1 8089267312\n",
      "cleaned \n",
      "vector1 8089267312\n"
     ]
    }
   ],
   "source": [
    "t0=time.time()\n",
    "print(\"Loading embeddings\")\n",
    "embeddings = load_vectors(\"../input/crawl-300d-2M.vec\")  \n",
    "t1 =time.time()\n",
    "total_time=t1-t0\n",
    "\n",
    "print(\"time to load\", total_time)\n",
    "print(\" \")\n",
    "t0=time.time()\n",
    "\n",
    "# Read trainig data\n",
    "df = pd.read_csv(\"../input/train_tiny/train.csv\")\n",
    "df = df[[\"Title\",\"BodyMarkdown\",\"OpenStatus\"]]\n",
    "y = df.OpenStatus.values\n",
    "\n",
    "\n",
    "t1 =time.time()\n",
    "total_time=t1-t0\n",
    "print(\"time to read\", total_time)\n",
    "print(\" \")\n",
    "\n",
    "# create a new column called fold and fill it with -1\n",
    "df[\"kfold\"] = -1\n",
    "\n",
    "t0=time.time()\n",
    "print(\"creating sentence embedding\")\n",
    "\n",
    "vector1 =[]\n",
    "\n",
    "print(\"the BodyMarkDown column embedding\")\n",
    "vector1 = get_vectors(vector1,df.BodyMarkdown.values, embeddings)\n",
    "vector1 = np.array(vector1)    \n",
    "print(\"1st column shape\",vector1.shape)\n",
    "\n",
    "# save vector1 and target\n",
    "save(\"../input/train_tiny/vector1.npy\",vector1)  \n",
    "save(\"../input/train_tiny/target.npy\",y)\n",
    "\n",
    "del vector1\n",
    "vector1 =[]\n",
    "t1 =time.time()\n",
    "total_time=t1-t0\n",
    "print(\"time to BodyMarkDown\", total_time)\n",
    "print(\" \")\n",
    "\n",
    "print(\"the Title column embedding\")\n",
    "t0=time.time()\n",
    "vector1 = get_vectors(vector1,df.Title.values,embeddings)\n",
    "vector1 = np.array(vector1)    \n",
    "print(\"2nd column shape\",vector1.shape)\n",
    "save(\"../input/train_tiny/vector2.npy\",vector1)\n",
    "t1 =time.time()\n",
    "total_time=t1-t0\n",
    "print(\"time to Title\", total_time)\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "local_vars = list(locals().items())\n",
    "for var, obj in local_vars:\n",
    "    if var=='embeddings' or var =='vector1':\n",
    "        print(var, sys.getsizeof(obj))\n",
    "        \n",
    "# clear up memory\n",
    "print(\"cleaned \")\n",
    "del embeddings\n",
    "\n",
    "local_vars = list(locals().items())\n",
    "for var, obj in local_vars:\n",
    "    if var=='embeddings' or var =='vector1':\n",
    "        print(var, sys.getsizeof(obj))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
