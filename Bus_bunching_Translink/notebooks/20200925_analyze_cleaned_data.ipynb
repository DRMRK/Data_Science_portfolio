{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze dataset processed in clean_BB_data.ipynb\n",
    "- Data file is in Cleaned_Trips2016_10.csv\n",
    "- Read data as feather format and load to pandas dataframe\n",
    "- We want to predict NextLegBunchingFlag, NextNextLegBunchingFlag, NextNextNextLegBunchingFlag  \n",
    "- Data is highly imbalanced. \n",
    "- Careful in choosing train test and validation data \n",
    "- Do not want to use random subset of data\n",
    "- Try stratified sampling.\n",
    "- Data has 138752 rows, considering it as medium dataset choose test:10%, validation:10% and train:80%.\n",
    "- Grid search for hyper parameter tuning.\n",
    "- Random search for hyperparameter tuning.\n",
    "- Saved trained models using joblib as 'Random_forest_optimized.joblib' and 'Random_forest_not_optimized.joblib'\n",
    "- Permutation importance for feature importance.\n",
    "- Using 54 original features obtained mcc_score of 0.8550, training time 34 minute.\n",
    "- Selecting top 8 features obtained mcc_score of 0.8577, training time 81 s.\n",
    "- Logistic Regression, used feature scalling.\n",
    "- Looked at threshold precision recall curve.\n",
    "- Wrote stuff in class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/DNN/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_path = os.path.abspath(os.path.join('..'))\n",
    "if current_path not in sys.path:\n",
    "    sys.path.append(current_path)\n",
    "    \n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helper.imports import *\n",
    "from helper.structured  import *\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from helper.plots_and_scores import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset \n",
    "data = pd.read_feather(f'{current_path}/data/processed/Cleaned_Trips2016_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_all(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['index'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['NextLegBunchingFlag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.NextLegBunchingFlag.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.NextNextLegBunchingFlag.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.NextNextNextLegBunchingFlag.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to numeric column and separate out the target variable\n",
    "X, y, nas  = proc_df(data, y_fld ='NextLegBunchingFlag',skip_flds=['NextNextLegBunchingFlag',\\\n",
    "                     'NextNextNextLegBunchingFlag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling for train, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test_valid, y_train, y_test_valid = train_test_split(X, y,  random_state =1, \\\n",
    "                                                                test_size=0.20,stratify = y)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test_valid, y_test_valid,  random_state =1, \\\n",
    "                                                                test_size=0.50,stratify = y_test_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at distribution of targets in train, test and validation set\n",
    "print('train:',np.unique(y_train,return_counts=True))\n",
    "print('test:',np.unique(y_test,return_counts=True))\n",
    "print('valid:',np.unique(y_valid,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "%time RFmodel = RandomForestClassifier(random_state=0,n_estimators=40,criterion='gini',\\\n",
    "                                       n_jobs=-1,oob_score=True)\n",
    "%time RFmodel.fit(X_train,y_train)\n",
    "#print_scores(RFmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= RFmodel.predict(X_train)\n",
    "y_pred_proba = RFmodel.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = plot_and_scores(y_train,y_pred, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.precision_recall_vs_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(y_train,y_pred_proba[:,1])\n",
    "plt.figure()\n",
    "plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "plt.plot(thresholds, precisions[: -1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[: -1], \"r--\", label=\"Recall\")\n",
    "plt.ylabel(\"Precision, Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "        #plt.ylim([0,1])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.display_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search parameters then try Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import fbeta_score\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFmodel = RandomForestClassifier(random_state=0,criterion='gini',\\\n",
    "                                       n_jobs=-1,oob_score=True,n_estimators=100)\n",
    "# Create the parameters list to tune, using a dictionary if needed.\n",
    "parameters = {'min_samples_leaf': [2,3],'max_features':[0.3,0.5],\\\n",
    "                    'max_samples':[0.6,0.8,1]}\n",
    "# Make an mcc_Metric scoring object using make_scorer()\n",
    "scorer = make_scorer(mcc_Metric, greater_is_better=True)\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "print(\"In instantiate now ....\")\n",
    "%time grid_obj = GridSearchCV(RFmodel, parameters, scoring = scorer)\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "print(\"In fit now ....\")\n",
    "%time grid_fit = grid_obj.fit(X_train, y_train)\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "# Make predictions using the unoptimized model\n",
    "print(\"In unoptimized fit now ....\")\n",
    "%time Unoptimizedpredictions = (RFmodel.fit(X_train, y_train)).predict(X_test)\n",
    "\n",
    "# Make predictions using the optimized and model\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "print('Results--------------\\n')\n",
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"MCC score on testing data: {:.4f}\".format(mcc_Metric(y_test, Unoptimizedpredictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, Unoptimizedpredictions, beta = 0.5)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final MCC score on the testing data: {:.4f}\".format(mcc_Metric(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "# Compute ROC curve and ROC area for each class\n",
    "def plot_ROC(y_test, best_predictions):\n",
    "    n_classes = 1\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test, best_predictions)\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), best_predictions.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[0])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for optimized model')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC(y_test, best_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time un_opt_model = RFmodel.fit(X_train, y_train)\n",
    "Unopt_pred_train =un_opt_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm, uniform\n",
    "from random import randint\n",
    "\n",
    "# create random forest classifier model\n",
    "rf_model = RandomForestClassifier(n_jobs=-1)\n",
    "model_params = dict(\n",
    "    # randomly sample numbers from 4 to 204 estimators\n",
    "    n_estimators=[randint(100,250)],\n",
    "    #n_estimators =[80,100],\n",
    "    #normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1\n",
    "    #max_features=truncnorm(a=0, b=1, loc=0.25, scale=0.1),\n",
    "    max_features = [0.3,0.5],\n",
    "    # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n",
    "    #min_samples_split=uniform(0.01, 0.399),\n",
    "    #uniform distribution from 0.5 to 0.99 (0.5 + 0.99)\n",
    "    #max_samples=uniform(0.5,0.49),\n",
    "    max_samples= [0.8,0.9],\n",
    "    min_samples_leaf=[2,3,5]\n",
    ")\n",
    "#Set up the random search estimator, this will train 5 models.\n",
    "clf = RandomizedSearchCV(rf_model, model_params, n_iter = 10,random_state=0,scoring = scorer)\n",
    "print(\"In fit now\")\n",
    "%time model_rs = clf.fit(X_train, y_train)\n",
    "# Get the best estimator\n",
    "best_clf_rs = model_rs.best_estimator_\n",
    "#Make predictions using the unoptimized\n",
    "#predictions_UO = (RFmodel.fit(X_train, y_train)).predict(X_test)\n",
    "#Make predictions using the optimized\n",
    "best_pred_rs_train = best_clf_rs.predict(X_train)\n",
    "best_pred_rs = best_clf_rs.predict(X_test)\n",
    "\n",
    "# Report the before-and-afterscores\n",
    "print('Results--------------\\n')\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"MCC score on training data: {:.4f}\".format(mcc_Metric(y_train, Unopt_pred_train)))\n",
    "print(\"MCC score on testing data: {:.4f}\".format(mcc_Metric(y_test, Unoptimizedpredictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, Unoptimizedpredictions, beta = 0.5)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final MCC score on the testing data: {:.4f}\".format(mcc_Metric(y_train, best_pred_rs_train)))\n",
    "print(\"Final MCC score on the testing data: {:.4f}\".format(mcc_Metric(y_test, best_pred_rs)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_pred_rs, beta = 0.5)))\n",
    "# Print the best parameters\n",
    "pprint(model_rs.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.scorer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC(y_test,best_predictions_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the models\n",
    "- Unoptimized model. \n",
    "- Optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model_rs,'Random_forest_optimized.joblib')\n",
    "joblib.dump(RFmodel,'Random_forest_not_optimized.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_RF = joblib.load('Random_forest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pd = load_RF.predict(X_test)\n",
    "print(\"Final MCC score on the testing data: {:.4f}\".format(mcc_Metric(y_test, y_pd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance.\n",
    "- Use permutation importance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(load_RF, X_test, y_test, n_repeats=10,random_state=0)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose top 8 features . For the rest of the analysis we will use this features for modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns[sorted_idx[-8:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features = X_test[['Temp', 'DwellTime', 'OnsAndOffsCompensated', 'ArriveLoadCompensated',\\\n",
    "       'ArriveDelay', 'StopName', 'HeadwayOffset', 'ActualHeadway']]\n",
    "\n",
    "X_train_features = X_train[['Temp', 'DwellTime', 'OnsAndOffsCompensated', 'ArriveLoadCompensated',\\\n",
    "       'ArriveDelay', 'StopName', 'HeadwayOffset', 'ActualHeadway']]\n",
    "\n",
    "X_valid_features = X_valid[['Temp', 'DwellTime', 'OnsAndOffsCompensated', 'ArriveLoadCompensated',\\\n",
    "       'ArriveDelay', 'StopName', 'HeadwayOffset', 'ActualHeadway']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model on the top 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def RandomtuneRF(X_tr, y_tr, X_test, y_test):\n",
    "    # This returns best_pred_train, best_pred_test, model\n",
    "    # create random forest classifier model\n",
    "    model = RandomForestClassifier(n_jobs=-1)\n",
    "    model_params = dict(\n",
    "    # randomly sample numbers from 4 to 204 estimators\n",
    "        n_estimators=[randint(100,200)],\n",
    "    #n_estimators =[80,100],\n",
    "    #normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1\n",
    "    #max_features=truncnorm(a=0, b=1, loc=0.25, scale=0.1),\n",
    "        max_features = [0.3,0.5],\n",
    "    # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n",
    "    #min_samples_split=uniform(0.01, 0.399),\n",
    "    #uniform distribution from 0.5 to 0.99 (0.5 + 0.99)\n",
    "    #max_samples=uniform(0.5,0.49),\n",
    "        max_samples= [0.8,0.9],\n",
    "        min_samples_leaf=[2,3,5]\n",
    "    )\n",
    "#Set up the random search estimator, this will train 5 models.\n",
    "    clf = RandomizedSearchCV(model, model_params, n_iter = 3,random_state=0,scoring = scorer)\n",
    "    print(\"In fit now--------\\n\")\n",
    "    t = time.process_time()\n",
    "    fit_model = clf.fit(X_tr, y_tr)\n",
    "    elapsed_time = time.process_time() - t\n",
    "    print(\"Time to fit: {:.4f} s\".format(elapsed_time))\n",
    "# Get the best estimator\n",
    "    best_clf = fit_model.best_estimator_\n",
    "#Make predictions using the unoptimized\n",
    "#predictions_UO = (RFmodel.fit(X_train, y_train)).predict(X_test)\n",
    "#Make predictions using the optimized\n",
    "    best_pred_train = best_clf.predict(X_tr)\n",
    "    best_pred_test = best_clf.predict(X_test)\n",
    "    return fit_model,best_pred_train, best_pred_test\n",
    "    # Print the best parameters\n",
    "    pprint(model_rs.best_estimator_.get_params())\n",
    "    \n",
    "def compare_models(y_train,y_test,Unopt_pred_train,Unoptimizedpredictions,best_pred_train,best_pred_test):\n",
    "#Report the before-and-afterscores\n",
    "    print('Results--------------\\n')\n",
    "    print(\"Unoptimized model\\n------\")\n",
    "    print(\"MCC score on training data: {:.4f}\".format(mcc_Metric(y_train, Unopt_pred_train)))\n",
    "    print(\"MCC score on testing data: {:.4f}\".format(mcc_Metric(y_test, Unoptimizedpredictions)))\n",
    "    print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, Unoptimizedpredictions, beta = 0.5)))\n",
    "    print(\"\\nOptimized Model\\n------\")\n",
    "    print(\"Final MCC score on the training data: {:.4f}\".format(mcc_Metric(y_train, best_pred_train)))\n",
    "    print(\"Final MCC score on the testing data: {:.4f}\".format(mcc_Metric(y_test, best_pred_test)))\n",
    "    print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_pred_test, beta = 0.5)))\n",
    "    plot_ROC(y_test,best_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model,best_pred_train, best_pred_test = RandomtuneRF(X_train_features, y_train, X_test_features, y_test)\n",
    "compare_models(y_train,y_test,Unopt_pred_train,Unoptimizedpredictions,best_pred_train,best_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(fit_model,'Random_forest_optimized_8_features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put everything in a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF_model:\n",
    "    \"\"\" Runs Random forest \"\"\"\n",
    "    def __init__(self):\n",
    "        self.random_forest = RandomForestClassifier(n_jobs=-1,criterion ='gini')\n",
    "    \n",
    "    def unoptimized_RF(self,X, y):\n",
    "    \"\"\" \n",
    "    Trains a model with default parameters and returns the model\n",
    "    Parameter\n",
    "    ---------\n",
    "    X : input features\n",
    "    y : target\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    un_opt_model: trained model\n",
    "    \n",
    "    \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        print(\"In unoptimized fit now--------\")\n",
    "        t = time.process_time()\n",
    "        un_opt_model = self.random_forest.fit(self.X, self.y)\n",
    "        elapsed_time = time.process_time() - t\n",
    "        print(\"Time taken to fit: {:.4f} s\".format(elapsed_time))\n",
    "        print('\\n')\n",
    "        #un_opt_pred =un_opt_model.predict(self.X)\n",
    "        return un_opt_model  \n",
    "    \n",
    "    def randomtuned_RF(self, X_train, y_train):\n",
    "    \"\"\" \n",
    "    Trains a model using random parameters and returns the model\n",
    "    Parameter\n",
    "    ---------\n",
    "    X : input features\n",
    "    y : target\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    best_clf : trained model\n",
    "    \n",
    "    \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        # create random forest classifier model\n",
    "        model = self.random_forest\n",
    "        model_params = dict(\n",
    "    # randomly sample numbers from 4 to 204 estimators\n",
    "            n_estimators=[randint(120,200)],\n",
    "    #n_estimators =[80,100],\n",
    "    #normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1\n",
    "    #max_features=truncnorm(a=0, b=1, loc=0.25, scale=0.1),\n",
    "            max_features = [0.3,0.5,0.8],\n",
    "    # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n",
    "    #min_samples_split=uniform(0.01, 0.399),\n",
    "    #uniform distribution from 0.5 to 0.99 (0.5 + 0.99)\n",
    "    #max_samples=uniform(0.5,0.49),\n",
    "            max_samples= [0.8,0.9,0.99],\n",
    "            min_samples_leaf=[2,3,5]\n",
    "        )\n",
    "#Set up the random search estimator, this will train 5 models.\n",
    "        clf = RandomizedSearchCV(model, model_params, n_iter = 5,random_state=0,scoring = scorer)\n",
    "        print(\"In randomtune fit now--------\")\n",
    "        t = time.process_time()\n",
    "        fit_model = clf.fit(self.X_train, self.y_train)\n",
    "        elapsed_time = time.process_time() - t\n",
    "        print(\"Time taken to fit: {:.4f} s\".format(elapsed_time))\n",
    "        print('\\n')\n",
    "        # Get the best estimator\n",
    "        best_clf = fit_model.best_estimator_\n",
    "        #Print the best parameters\n",
    "        #pprint(fit_model.best_estimator_.get_params())\n",
    "        return best_clf\n",
    "    \n",
    "\n",
    "        # Compute ROC curve and ROC area for each class\n",
    "    def plot_ROC(self,y,y_pred,name):\n",
    "        self.y = y\n",
    "        self.y_pred = y_pred\n",
    "        self.name = name\n",
    "        n_classes = 1\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(self.y, self.y_pred)\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(self.y.ravel(), self.y_pred.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[0])\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic %s'%name)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "    def scores(self,y,y_pred):\n",
    "        #Takes prdictions from two models and outputs the MCC score and f beta score\n",
    "        self.y = y\n",
    "        self.y_pred = y_pred\n",
    "      #Report the before-and-afterscores\n",
    "        print(\"Accuracy score: {:.4f}\".format(accuracy_score(self.y, self.y_pred)))\n",
    "        print(\"MCC score: {:.4f}\".format(mcc_Metric(self.y,self.y_pred)))\n",
    "        print(\"F-score: {:.4f}\".format(fbeta_score(self.y, self.y_pred, beta = 0.5)))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we return the models\n",
    "rf_model = RF_model()\n",
    "# First train a base model with default parameters.\n",
    "base_model = rf_model.unoptimized_RF(X_train_features,y_train)\n",
    "# Now train a model to optimize parameters.\n",
    "optimized_model = rf_model.randomtuned_RF(X_train_features,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimized model\n",
    "y_pred_train = optimized_model.predict(X_train_features)\n",
    "y_pred_test = optimized_model.predict(X_test_features)\n",
    "\n",
    "y_pred_train_base = base_model.predict(X_train_features)\n",
    "y_pred_test_base = base_model.predict(X_test_features)\n",
    "#Try optimized model\n",
    "print(\"Optimized model-------\")\n",
    "print(\"Train score:-->\")\n",
    "rf_model.scores(y_train,y_pred_train)\n",
    "#rf_model.plot_ROC(y_train[:2000],y_pred_train,'optimized model, train')\n",
    "print(\"\\n Test score:-->\")\n",
    "rf_model.scores(y_test,y_pred_test)\n",
    "rf_model.plot_ROC(y_test,y_pred_test,'optimized model, test')\n",
    "print(\"Not optimized model--------\")\n",
    "print(\"Train score:-->\")\n",
    "#Try not optimized model\n",
    "rf_model.scores(y_train,y_pred_train_base)\n",
    "#rf_model.plot_ROC(y_train[:2000],y_pred_train_base,'Not optimized model, train')\n",
    "print(\"\\n Test score:-->\")\n",
    "rf_model.scores(y_test,y_pred_test_base)\n",
    "rf_model.plot_ROC(y_test,y_pred_test_base,'Not optimized model, test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(optimized_model,'Random_forest_optimized_8_features.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try Logisitic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "class log_reg_model():\n",
    "    \n",
    "    def __init__(self, X,y):\n",
    "        self.log_reg = LogisticRegression(max_iter=1000, C=100000,penalty = 'l2')\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def unoptimized_LR(self):\n",
    "       #takes in X and y and trains a model with default parameters and returns the model\n",
    "        print(\"In unoptimized fit now--------\")\n",
    "        t = time.process_time()\n",
    "        un_opt_model = self.log_reg.fit(self.X, self.y)\n",
    "        elapsed_time = time.process_time() - t\n",
    "        print(\"Time taken to fit: {:.4f} s\".format(elapsed_time))\n",
    "        print('\\n')\n",
    "        #un_opt_pred =un_opt_model.predict(self.X)\n",
    "        return un_opt_model\n",
    "    \n",
    "    \n",
    "class plot_and_scores():\n",
    "    \"\"\" \n",
    "    Input \n",
    "    ---------\n",
    "    y : actual target values\n",
    "    y_pred : predicted target  values \n",
    "    y_pred_proba : predicted probabilities  \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,y,y_pred, y_pred_proba):\n",
    "        self.y = y\n",
    "        self.y_pred = y_pred\n",
    "        self.y_pred_proba = y_pred_proba\n",
    "        \n",
    "    def plot_ROC(self,name):\n",
    "        self.name = name\n",
    "        n_classes = 1\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(self.y, self.y_pred)\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        # Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(self.y.ravel(), self.y_pred.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[0])\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver operating characteristic %s'%name)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "    def print_scores(self):\n",
    "        print(\"Accuracy score: {:.4f}\".format(accuracy_score(self.y, self.y_pred)))\n",
    "        print(\"MCC score: {:.4f}\".format(mcc_Metric(self.y,self.y_pred)))\n",
    "        print(\"F-score: {:.4f}\".format(fbeta_score(self.y, self.y_pred, beta = 0.5)))\n",
    "    \n",
    "    \n",
    "    def mcc_Metric(self):\n",
    "        \n",
    "        cf_matrix=confusion_matrix(self.y,self.y_preds)\n",
    "        TP = cf_matrix[0][0]\n",
    "        TN = cf_matrix[1][1]\n",
    "        FN = cf_matrix[0][1]\n",
    "        FP = cf_matrix[1][0]\n",
    "        N = TN + TP + FN + FP \n",
    "        S = (TP + FN)/N\n",
    "        P = (TP + FP)/N\n",
    "        num = (TP /N) - (S*P)\n",
    "        deno = np.sqrt(P*S*(1-S)*(1-P))\n",
    "        \n",
    "        #Need to avoide division by zero\n",
    "        return weird_division(num,deno)\n",
    "    \n",
    "    def display_confusion_matrix(self):\n",
    "        cm = confusion_matrix(self.y, self.y_pred)\n",
    "        ConfusionMatrixDisplay(cm).plot() \n",
    "        \n",
    "\n",
    "    def weird_division(self,n, d):\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        return self.n / self.d if self.d else 0  \n",
    "   \n",
    "   \n",
    "    def precision_recall_vs_threshold(self):\n",
    "        precisions, recalls, thresholds = precision_recall_curve(self.y,self.y_pred_proba[:,1])\n",
    "        plt.figure()\n",
    "        plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "        plt.plot(thresholds, precisions[: -1], \"b--\", label=\"Precision\")\n",
    "        plt.plot(thresholds, recalls[: -1], \"r--\", label=\"Recall\")\n",
    "        plt.ylabel(\"Precision, Recall\")\n",
    "        plt.xlabel(\"Threshold\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.ylim([0,1])\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X1 = StandardScaler().fit_transform(X_train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we return the models\n",
    "log_r_model = log_reg_model(X1,y_train)\n",
    "# First train a base model with default parameters.\n",
    "base_model_LR = log_r_model.unoptimized_LR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "y_pred_LR = base_model_LR.predict(X1)\n",
    "y_pred_proba = base_model_LR.predict_proba(X1)\n",
    "#y_pred_test_base = base_model.predict(X_test_features)\n",
    "ps = plot_and_scores(y_train,y_pred_LR,y_pred_proba)\n",
    "ps.print_scores()\n",
    "ps.plot_ROC('test')\n",
    "ps.display_confusion_matrix()\n",
    "ps.precision_recall_vs_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_LR_prob = base_model_LR.predict_proba(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train,y_pred_LR_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('precision shape',precisions.shape, 'threshold shape',thresholds.shape,'recalls shape',recalls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "111001-110752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probs_y is a 2-D array of probability of being labeled as 0 (first column of #array) vs 1 (2nd column in array)\n",
    "\n",
    "#precision, recall, thresholds = precision_recall_curve(y_train,y_pred_LR_prob[:,1]) \n",
    "#retrieve probability of being 1(in second column of probs_y)\n",
    "#pr_auc = metrics.auc(recall, precision)\n",
    "plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "plt.plot(thresholds, precisions[: -1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[: -1], \"r--\", label=\"Recall\")\n",
    "plt.ylabel(\"Precision, Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    #plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "#plt.plot([0.4368, 0.4368], [0., 0.9], \"r:\")\n",
    "#plt.plot([0.0, 0.4368], [0.9, 0.9], \"r:\")\n",
    "#plt.plot([0.4368], [0.9], \"ro\")\n",
    "#save_fig(\"precision_vs_recall_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot scatter matrix for all features\n",
    "#params = [\"HeadwayOffset\",\"Temp\", \"Windspeed\",\"Dwell Time\"]\n",
    "#pd.scatter_matrix(X_train, alpha = 0.3, figsize = (14,8), diagonal = 'kde');\n",
    "#plt.savefig(\"scatter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # TODO: Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train[:sample_size],y_train[:sample_size])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # TODO: Get the predictions on the test set(X_test),\n",
    "    #       then get predictions on the first 300 training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # TODO: Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # TODO: Compute accuracy on the first 300 training samples which is y_train[:300]\n",
    "    results['acc_train'] = accuracy_score(y_train[:300], predictions_train[:300])\n",
    "        \n",
    "    # TODO: Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(y_test, predictions_test)\n",
    "    \n",
    "    # TODO: Compute F-score on the the first 300 training samples using fbeta_score()\n",
    "    results['f_train'] = fbeta_score(y_train[:300], predictions_train, beta=0.5)\n",
    "        \n",
    "    # TODO: Compute F-score on the test set which is y_test\n",
    "    results['f_test'] = fbeta_score(y_test, predictions_test, beta=0.5)\n",
    "       \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visuals as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the three supervised learning models from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "\n",
    "# TODO: Initialize the three models\n",
    "clf_A = LogisticRegression(random_state=0)\n",
    "clf_B = DecisionTreeClassifier(random_state=0)\n",
    "clf_C = AdaBoostClassifier(random_state=0)\n",
    "\n",
    "# TODO: Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "# HINT: samples_100 is the entire training set i.e. len(y_train)\n",
    "# HINT: samples_10 is 10% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n",
    "# HINT: samples_1 is 1% of samples_100 (ensure to set the count of the values to be `int` and not `float`)\n",
    "samples_100 = len(X_train)\n",
    "samples_10 = int(0.1 * len(X_train))\n",
    "samples_1 = int(0.01 * len(X_train))\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, samples, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "vs.evaluate(results, accuracy, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(labels[:300],bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visuals as vs\n",
    "importances = RFmodel.feature_importances_\n",
    "\n",
    "# Plot\n",
    "vs.feature_plot(importances, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def fit_model(X, y):\n",
    "    \"\"\" Performs grid search over the 'max_depth' parameter for a \n",
    "        decision tree regressor trained on the input data [X, y]. \"\"\"\n",
    "    \n",
    "    # Create cross-validation sets from the training data\n",
    "    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    cv_sets = ShuffleSplit(n_splits=10, test_size = 0.20, random_state=None)\n",
    "\n",
    "    # TODO: Create a decision tree regressor object\n",
    "    classifier = RandomForestClassifier()\n",
    "\n",
    "    # TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
    "    params = {'max_depth': range(1,10), 'n_estimators':[10,50,100,200]}\n",
    "\n",
    "    # TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \n",
    "    scoring_fnc = make_scorer(accuracy_score)\n",
    "\n",
    "    # TODO: Create the grid search cv object --> GridSearchCV()\n",
    "    # Make sure to include the right parameters in the object:\n",
    "    # (estimator, param_grid, scoring, cv) which have values 'regressor', 'params', 'scoring_fnc', and 'cv_sets' respectively.\n",
    "    grid = GridSearchCV(classifier, params, scoring = scoring_fnc, cv = cv_sets)\n",
    "\n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg = fit_model(X_train, y_train.values.ravel())\n",
    "\n",
    "# Produce the value for 'max_depth'\n",
    "\n",
    "\n",
    "print (\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))\n",
    "print (\"Parameter 'n_estimators' is {} for the optimal model.\".format(reg.get_params()['n_estimators']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
