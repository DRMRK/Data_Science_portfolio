{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on training data: 0.9375\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import the necessary python libraries \n",
    "\"\"\" \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "# Load the titanic dataset \n",
    "data = pd.read_csv(\"Datasets/titanic_data.csv\")\n",
    "#Replace NANs with mean values.\n",
    "data['Age'] = data['Age'].fillna(data['Age'].mean())\n",
    "\n",
    "# Encode the gender categories to numerical values and save it as a new column 'Gender'.\n",
    "lb_make = LabelEncoder()\n",
    "data['Gender'] = lb_make.fit_transform(data['Sex'])\n",
    "\n",
    "# Create a sample dataset to test our Decision Tree algorithm and to compare the results with the ones obtained from sklearn.\n",
    "sample_dataset = data.head(20)\n",
    "\n",
    "# Only considering categorical variables for now.\n",
    "features = sample_dataset[['Pclass', 'Gender', 'Age','SibSp','Parch','Fare']]\n",
    "features_cat = sample_dataset[['Pclass', 'Gender','Parch']]\n",
    "labels = sample_dataset[['Survived']]\n",
    "\n",
    "# Split the 'categorical features' and 'target label' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_cat,labels, test_size = 0.2, random_state = 0)\n",
    "train_data = pd.concat([X_train, y_train], axis =1)\n",
    "test_data = pd.concat([X_test, y_test], axis =1)\n",
    "# Save training and test sample dataset.\n",
    "train_data.to_csv('Datasets/training_sample.csv')\n",
    "test_data.to_csv('Datasets/test_sample.csv')\n",
    "\n",
    "# Fit the decision tree classifier from sklearn and make predictions and calculate accuracy and F-score on training and test datasets. \n",
    "model = DecisionTreeClassifier(random_state=0,criterion='entropy')\n",
    "model.fit(X_train,y_train)\n",
    "predictions_train=model.predict(X_train)\n",
    "predictions=model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score on training data: {:.4f}\".format(accuracy_score(y_train, predictions_train)))\n",
    "#print(\"F-score on training data: {:.4f}\".format(fbeta_score(y_train, predictions_train, beta = 0.5)))\n",
    "#print(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "#print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a decision tree from scratch and compare the results with the results obtained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the 'entropy' method to determine the best feature used for splitting at each node. The procedure with equations will be added here. First we calculate entropy of the whole dataset using target labels (entropy_parent). Then the weighted entropy for each feature in the dataset is calculated and subtracted from the parent entropy to get the information gain for each feature. The feature which has the highest information gain is chosen for splitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the necessary python libraries \n",
    "\"\"\" \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the sample training dataset \n",
    "#train_data = pd.read_csv(\"training_sample.csv\")\n",
    "##################################################################################################################\n",
    "def entropy_parent(data):\n",
    "    \"\"\"\n",
    "    Calculate entropy of the training set using the target labels. \n",
    "    The parameter data = training dataset\n",
    "    \"\"\"\n",
    "    en = 0\n",
    "    target = data.keys()[-1]\n",
    "    target_labels = data[target].unique()\n",
    "    for t_label in target_labels:\n",
    "        p_class = data[target].value_counts()[t_label]/len(data[target])\n",
    "        en += - (p_class * np.log2(p_class))\n",
    "    return en\n",
    "##################################################################################################################\n",
    "\n",
    "##################################################################################################################\n",
    "def entropy_child(data,child):\n",
    "    \"\"\"\n",
    "    Calculate the weighted entropy for the feature in the training data. This function takes two parameters:\n",
    "    1. data = The dataset for which we need to calculate the entropy of each feature\n",
    "    2. child = The feature in the dataset for which the entropy should be calculated \n",
    "    \"\"\"\n",
    "    en_child_weighted = 0 \n",
    "    target = data.keys()[-1]\n",
    "    target_labels = data[target].unique()\n",
    "    child_labels = data[child].unique()  # get the unique labels from the feature column selected as child\n",
    "    for c_label in child_labels:\n",
    "        en_child_feature = 0\n",
    "        for t_label in target_labels:\n",
    "            p_class_num = len(data[child][data[child]==c_label][data[target] == t_label]) \n",
    "            p_class_den = len(data[child][data[child]==c_label])\n",
    "            if (p_class_num) != 0:\n",
    "                p_class = p_class_num/p_class_den\n",
    "                en_child_feature += - p_class * np.log2(p_class) \n",
    "            else:\n",
    "                en_child_feature = 0.0\n",
    "    \n",
    "        weight = p_class_den/len(data)\n",
    "        en_child_weighted += (weight*en_child_feature)   \n",
    "    return en_child_weighted\n",
    "##################################################################################################################\n",
    "\n",
    "##################################################################################################################\n",
    "def select_max_IG_feature(data):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for each feature in the dataset and determine the feature with maximum gain. \n",
    "    1. data = The dataset for which we need to determine the feature which has maximum gain\n",
    "    \"\"\"\n",
    "    ig = []\n",
    "    for child in data.keys()[:-1]:\n",
    "        en_parent = entropy_parent(data)  # Calculate parent entropy\n",
    "        en_child = entropy_child(data,child) # Calculate weighted entropy for each feature in the training data. \n",
    "        ig_child = en_parent - en_child\n",
    "        ig.append(ig_child)\n",
    "    return data.keys()[:-1][np.argmax(ig)]\n",
    "##################################################################################################################\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "def build_DTree(data,all_data,features_col,target_col, parent_node_label=None):\n",
    "    \"\"\"\n",
    "    Building the Decision Tree: This function takes five paramters:\n",
    "    1. data = The data for which we want to build the tree. In the first run this data is the whole training dataset \n",
    "    2. all_data = This is the whole training dataset. It is used to calculate the target feature value when the dataset\n",
    "    given by the first parameter is empty\n",
    "    3. features_col = the features of the dataset included at the time of building the tree. In the first run all features\n",
    "    are included. When the data is split at     each node, the corresponding feature is removed from the dataset\n",
    "    4. target_col = the name of the target column\n",
    "    5. parent_node_label = This is the value or label of the target feature value of the parent node for a specific node. \n",
    "    While growing the tree, if no features are left in the features_col, we return the target feature value of the direct \n",
    "    parent node.    \n",
    "    \"\"\"   \n",
    "    #stopping criteria\n",
    "    if len(np.unique(data[target_col]))<=1:\n",
    "        return np.unique(data[target_col])[0]\n",
    "    #if dataset is empty return the target feature label value from the whole training dataset.\n",
    "    elif len(data)==0:\n",
    "        return np.unique(all_data[target_col])[np.argmax(np.unique(all_data[target_col],return_counts=True)[1])]\n",
    "    elif len(features_col)==0:\n",
    "        return parent_node_label\n",
    "    #Grow the tree\n",
    "    #Default value for this node, the nodeclass value of the current\n",
    "    #node\n",
    "    parent_node_label=np.unique(data[target_col])[np.argmax(np.unique(data[target_col],return_counts=True)[1])]\n",
    "    #select the feature that best splits the dataset\n",
    "    best_feature=select_max_IG_feature(data)\n",
    "    tree={best_feature:{}}\n",
    "    #Remove the feature with the best inforamtion gain from the feature space\n",
    "    features_col = [i for i in features_col if i != best_feature]\n",
    "    for value in np.unique(data[best_feature]):\n",
    "        value = value\n",
    "        #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "        sub_data = data.where(data[best_feature] == value).dropna()\n",
    "        #print(sub_data)\n",
    "        target_col = data.keys()[-1]\n",
    "        #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "        subtree = build_DTree(sub_data,train_data,features_col,target_col,parent_node_label)\n",
    "        #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "        tree[best_feature][value] = subtree\n",
    "    return(tree) \n",
    "    \n",
    "##################################################################################################################\n",
    "##################################################################################################################\n",
    "\n",
    "def predict(test_example,tree):\n",
    "    \"\"\"\n",
    "    Predict the label for an unknown example. The parameters are: \n",
    "    1. test_example = A row from the test set \n",
    "    2. tree = the saved decision tree using the function build_DTree\n",
    "    \"\"\"\n",
    "    for nodes in tree.keys():      #Go into each node to check where the test example belongs.  \n",
    "        feature_value = test_example[nodes]\n",
    "        tree = tree[nodes][feature_value]\n",
    "        prediction = 0\n",
    "            \n",
    "        if type(tree) is dict:\n",
    "            prediction = predict(test_example, tree)\n",
    "        else:\n",
    "            prediction = tree\n",
    "            break;                            \n",
    "        \n",
    "    return prediction\n",
    "##################################################################################################################\n",
    "\n",
    "##################################################################################################################\n",
    "def dt_accuracy(test_data,tree):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the Decision Tree alogorithm by comparing the predicted labels with the actual labels.\n",
    "    The parameters are: \n",
    "    1. test_data = test dataset. Here the training dataset is the test dataset.\n",
    "    2. tree = the saved decision tree using the function build_DTree\n",
    "    \"\"\"\n",
    "    target=test_data.keys()[-1]\n",
    "    actual_labels=train_data[[target]].reset_index(drop=True)\n",
    "    test_data_to_use=train_data.drop([target],axis=1)\n",
    "    pred = pd.DataFrame(columns=[\"predicted\"])\n",
    "    count=0\n",
    "    for i in range(0,len(test_data_to_use)):\n",
    "        test_example = test_data_to_use.iloc[i]\n",
    "        pred.loc[i,'predicted'] = predict(test_example,tree)\n",
    "        if (pred['predicted'][i] == actual_labels['Survived'][i]):\n",
    "            count+=1\n",
    "    accuracy = count/len(test_data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gender': {0: {'Pclass': {1.0: 1.0,\n",
       "    2.0: 1.0,\n",
       "    3.0: {'Parch': {0.0: 1.0, 1.0: 1.0}}}},\n",
       "  1: {'Pclass': {1.0: 0.0, 2.0: 1.0, 3.0: 0.0}}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = build_DTree(train_data,train_data,features_col=train_data.keys()[:-1],target_col=train_data.keys()[-1], parent_node_label=None)\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_accuracy(train_data,tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score matches with the one with sklearn for this sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
